{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yhng2525/7714P-Project/blob/main/L5/explainable_cnn.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Explainable AI\n",
        "\n",
        "This lab exercises explores using techniques such as Grad-CAM and LIME to explain the output of CNN image model."
      ],
      "metadata": {
        "id": "kPj3iRNJVQzf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 1: Grad-CAM\n",
        "\n",
        "In this exercise, we will use Grad-CAM to visualize what features are important in influencing the model prediction.\n",
        "\n",
        "We will be using [pytorch-gradcam](https://github.com/jacobgil/pytorch-grad-cam) package for this exercise"
      ],
      "metadata": {
        "id": "ACHFKXnE52rV"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "opoNZRY6jJHZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ce3e8c42-33e7-42a8-a5e1-3f3614fd3ec4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m45.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for grad-cam (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "%pip -q install grad-cam"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from PIL import Image\n",
        "import torch\n",
        "import os\n",
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from torchvision import models\n",
        "import torchvision.transforms as transforms\n",
        "from pytorch_grad_cam import GradCAM\n",
        "from pytorch_grad_cam.utils.image import (\n",
        "    show_cam_on_image, deprocess_image, preprocess_image\n",
        ")\n",
        "from pytorch_grad_cam.utils.model_targets import ClassifierOutputTarget, ClassifierOutputReST"
      ],
      "metadata": {
        "id": "NWWXjaV5j89i"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will load the image using cv2 library. We scale the image to be between 0 and 1.0 and further process the image to have the means and std deviations required for resnet as we will be using ResNet for our image classification task.\n",
        "\n",
        "Different pretrained network will have different ways to process the image (rescaling, resizing, etc).\n",
        "\n",
        "You can find different transformation that is required by looking at the `transform()` method bundled with the weights.\n",
        "\n",
        "```python\n",
        "\n",
        "from torchvision.models import resnet50, ResNet50_Weights\n",
        "\n",
        "weights = ResNet50_Weights.DEFAULT\n",
        "preprocess = weights.transforms()\n",
        "\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "aOkoZZLJ7APr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torchvision.models import resnet50, ResNet50_Weights\n",
        "\n",
        "weights = ResNet50_Weights.DEFAULT\n",
        "preprocess = weights.transforms()"
      ],
      "metadata": {
        "id": "VRGg7bujgfK7"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://github.com/nyp-sit/iti121-2025s2/blob/main/L5/cockatoo.jpeg?raw=true -O cockatoo.jpeg\n",
        "\n",
        "image = Image.open('cockatoo.jpeg')\n",
        "plt.imshow(image)"
      ],
      "metadata": {
        "id": "uy2ALdc1WivC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#  reverse channel order from BGR to RGB\n",
        "rgb_img = cv2.imread('cockatoo.jpeg', 1)[:, :, ::-1]\n",
        "rgb_img = np.float32(rgb_img) / 255\n",
        "input_tensor = preprocess_image(rgb_img,\n",
        "                                    mean=[0.485, 0.456, 0.406],\n",
        "                                    std=[0.229, 0.224, 0.225]).to(\"cpu\")"
      ],
      "metadata": {
        "id": "P7v2NrWNCezr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will use the pretrained network Resnet50 for predicting the image.  \n",
        "\n",
        "You will need to choose the target layer you want to compute the visualization for.\n",
        "Usually this will be the last convolutional layer in the model.\n",
        "Some common choices can be:\n",
        "- Resnet18 and 50: model.layer4\n",
        "- VGG, densenet161: model.features[-1]\n",
        "- mnasnet1_0: model.layers[-1]\n",
        "\n",
        "You can print the model to help chose the layer\n",
        "You can pass a list with several target layers,\n",
        "in that case the CAMs will be computed per layer and then aggregated."
      ],
      "metadata": {
        "id": "DnjD1J8La-g4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# specify the output directory to write the visualization to\n",
        "output_dir = \"output\"\n",
        "output_file = \"GradCam_cam.jpg\"\n",
        "\n",
        "# use CPU for computation\n",
        "device = torch.device(\"cpu\")"
      ],
      "metadata": {
        "id": "AWhWOWNJcAwC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = models.resnet50(pretrained=True).to(device).eval()\n",
        "target_layers = [model.layer4]"
      ],
      "metadata": {
        "id": "EZsOcQkmolKP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# targets = None    # If targets is None, the highest scoring category (for every member in the batch) will be used.\n",
        "targets = [ClassifierOutputTarget(89)]   # Take the gradient of the score for class 281 w.r.t. the convolutional activations.”\n",
        "\n",
        "cam_algorithm = GradCAM\n",
        "with cam_algorithm(model=model,\n",
        "                    target_layers=target_layers) as cam:\n",
        "\n",
        "    # cam.batch_size = 32\n",
        "    grayscale_cam = cam(input_tensor=input_tensor,\n",
        "                        targets=targets,\n",
        "                        aug_smooth=True,  # Apply test time augmentation to smooth the CAM\n",
        "                        eigen_smooth=True) # Reduce noise by taking the first principle component\n",
        "\n",
        "    # heat map\n",
        "    grayscale_cam = grayscale_cam[0, :]\n",
        "\n",
        "    # overlays the Grad-CAM heatmap (grayscale intensity) on top of the original RGB image — so you can visually see where the model is focusing.\n",
        "    cam_image = show_cam_on_image(rgb_img, grayscale_cam, use_rgb=True)\n",
        "\n",
        "    # OpenCV internally represents images in BGR order by default.\n",
        "    cam_image = cv2.cvtColor(cam_image, cv2.COLOR_RGB2BGR)\n",
        "\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "cam_output_path = os.path.join(output_dir, output_file)\n",
        "\n",
        "cv2.imwrite(cam_output_path, cam_image)"
      ],
      "metadata": {
        "id": "yDgFSSACo9-m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's display the resultant viualization."
      ],
      "metadata": {
        "id": "bGBkkaaQigkc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "img = plt.imread(cam_output_path)\n",
        "plt.imshow(img)"
      ],
      "metadata": {
        "id": "utJTlvEuAyag"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 2: LIME\n",
        "\n",
        "In this part of the exercise, we will use LIME, a perturbation, black-box technique to explain the output of the image model."
      ],
      "metadata": {
        "id": "0oEgZv4YV7KF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%pip -q install lime"
      ],
      "metadata": {
        "id": "jL599RwiWHf-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from lime import lime_image\n",
        "from skimage.segmentation import mark_boundaries\n",
        "from torchvision import models, transforms\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# Load pretrained model\n",
        "model = models.resnet18(pretrained=True).to(device).eval()"
      ],
      "metadata": {
        "id": "BSI4Ke8FWN2u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Preprocess image\n",
        "\n",
        "Before passing the image to model, we need to preprocess image (transform, resize, etc) to what the model expected during its training.  For example, resnet expects the images to have mean of (0.485, 0.456, 0.406) and std deviation of (0.229, 0.224, 0.225), for each channel."
      ],
      "metadata": {
        "id": "dvzkM4wrW2of"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Preprocess the image\n",
        "preprocess = transforms.Compose([\n",
        "    # transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                         std=[0.229, 0.224, 0.225])\n",
        "])\n"
      ],
      "metadata": {
        "id": "Hjd2tQFGXCS2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We need to define a classifier function that LIME can use to get the predicted probabilities"
      ],
      "metadata": {
        "id": "J739HlvOXNKI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def batch_predict(images):\n",
        "    \"\"\"Convert numpy arrays to tensor batch and predict\"\"\"\n",
        "    batch = torch.stack([preprocess(Image.fromarray(img.astype('uint8')))\n",
        "                         for img in images], dim=0).to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        logits = model(batch)\n",
        "        probs = torch.nn.functional.softmax(logits, dim=1)\n",
        "\n",
        "    return probs.cpu().numpy()"
      ],
      "metadata": {
        "id": "1B07w00hXH3n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now will use the LimeImageExplainer to introduce perturbations (masking off regions) and train a simple linear model (surrogate model) to predict feature importance.\n",
        "\n",
        "`num_samples` tells LIME how many perturbed versions of the input image to generate when building its local surrogate model.  Higher num_samples will provide smoother and more stable explanations, at the expense of longer runtime."
      ],
      "metadata": {
        "id": "bZO_hmLyXQ5M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load example image\n",
        "img = Image.open(\"cockatoo.jpeg\").convert(\"RGB\")\n",
        "\n",
        "# Initialize LIME explainer\n",
        "explainer = lime_image.LimeImageExplainer()\n",
        "\n",
        "# Explain a prediction\n",
        "explanation = explainer.explain_instance(\n",
        "    np.array(img),\n",
        "    classifier_fn=batch_predict,\n",
        "    top_labels=1,  # LIME will only explain the top predicted label (the class with highest probability).\n",
        "    hide_color=0, # When LIME “hides” a superpixel, it replaces its pixels with this value (color). In this case, it is black\n",
        "    num_samples=1000 # Number of perturbed samples (versions of the image) to generate.\n",
        ")\n"
      ],
      "metadata": {
        "id": "-NloEW5CXY7w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now let's visualize which regions are more important for the prediction."
      ],
      "metadata": {
        "id": "UmSHeR-HX4hU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualize result\n",
        "from matplotlib import pyplot as plt\n",
        "temp, mask = explanation.get_image_and_mask(\n",
        "    label=explanation.top_labels[0],  # which class to explain\n",
        "    positive_only=True, # Show only features (superpixels) that increase the probability of that class\n",
        "    hide_rest=True, # If True, hide non-important regions (fill them with gray/black); if False, keep the full image visible\n",
        "    num_features=5, # Number of most influential superpixels to highlight\n",
        "    min_weight=0.0 # Minimum importance threshold\n",
        ")\n",
        "\n",
        "plt.imshow(mark_boundaries(temp / 255.0, mask))\n",
        "plt.title(\"LIME Explanation\")\n",
        "plt.axis(\"off\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "-jSHI3m5X3oj"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}